{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __flatten_child(trees):\n",
    "    flat = []\n",
    "    #flat.extend(map(lambda x: {\"id\": x[\"id\"], \"text\": x[\"text\"], \"insult\": int(x[\"insult\"]), \"not insult\": int(not x[\"insult\"])}, filter(lambda x: \"insult\"  in x, trees)))\n",
    "    flat.extend(map(lambda x: {\"id\": x[\"id\"], \"text\": x[\"text\"], \"insult\": int(x[\"insult\"])}, filter(lambda x: \"insult\" in x, trees)))\n",
    "    for obj in trees:\n",
    "        if \"children\" in obj:\n",
    "            flat.extend(__flatten_child(obj[\"children\"]))\n",
    "    return flat\n",
    "\n",
    "def __flatten(trees):\n",
    "    flat = []\n",
    "    for root in trees:\n",
    "        obj = root[\"root\"]\n",
    "        if \"children\" in obj:\n",
    "            flat.extend(__flatten_child(obj[\"children\"]))\n",
    "    return flat\n",
    "\n",
    "def __split_corp_label(labeled_discussions):\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    for obj in __flatten(labeled_discussions):\n",
    "        if \"insult\" in obj:\n",
    "            corpus.append({\"id\": obj[\"id\"], \"text\": obj[\"text\"]})\n",
    "            #labels.append({\"id\": obj[\"id\"], \"insult\": int(obj[\"insult\"]), \"not insult\": int(not obj[\"insult\"])})\n",
    "            labels.append({\"id\": obj[\"id\"], \"insult\": int(obj[\"insult\"])})\n",
    "    \n",
    "    return corpus, labels\n",
    "\n",
    "#EMBEDDING_FILE = 'C:/fasttext/cc.ru.300.vec'\n",
    "EMBEDDING_FILE = 'C:/fasttext/fasttext300.bin'\n",
    "\n",
    "\n",
    "\n",
    "test_data = json.load(open(\"discussions_tpc_2015/modis/discussions.json\", encoding=\"utf8\"))\n",
    "train_data = json.load(open(\"discussions_tpc_2015/students/discussions.json\", encoding=\"utf8\"))\n",
    "\n",
    "train = pd.DataFrame.from_records(__flatten(train_data))\n",
    "test = pd.DataFrame.from_records(__flatten(test_data))\n",
    "#submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "# import contractions\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class TextCleaningUtils:\n",
    "    '''\n",
    "        This class contains implementations of various text cleaning operations (Static Methods)\n",
    "    '''\n",
    "\n",
    "\n",
    "    cleaning_regex_map = {\n",
    "        'web_links': r'(?i)(?:(?:http(?:s)?:)|(?:www\\.))\\S+',\n",
    "        'special_chars': r'[^a-zA-Zа-яА-ЯёЁ0-9\\s\\.,!?;:]+',\n",
    "        'redundant_spaces': r'\\s\\s+',\n",
    "        'redundant_newlines': r'[\\r|\\n|\\r\\n]+',\n",
    "        'twitter_handles': r'[#@]\\S+',\n",
    "        'punctuations': r'[\\.,!?;:]+'\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text_from_regex(text, text_clean_regex):\n",
    "        '''\n",
    "            Follow a particular cleaning expression, provided\n",
    "            as an input by an user to clean the text.\n",
    "        '''\n",
    "\n",
    "        text = text_clean_regex.sub(' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def strip_html(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_special_chars(text):\n",
    "        '''\n",
    "            Replace any special character provided as default,\n",
    "            which is present in the text with space\n",
    "        '''\n",
    "\n",
    "        special_chars_regex = re.compile(TextCleaningUtils.cleaning_regex_map['special_chars'])\n",
    "        text = TextCleaningUtils.clean_text_from_regex(text, special_chars_regex)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_redundant_spaces(text):\n",
    "        '''\n",
    "            Remove any redundant space provided as default,\n",
    "            that is present in the text.\n",
    "        '''\n",
    "\n",
    "        redundant_spaces_regex = re.compile(\n",
    "            TextCleaningUtils.cleaning_regex_map['redundant_spaces'])\n",
    "        text = TextCleaningUtils.clean_text_from_regex(text, redundant_spaces_regex)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_web_links(text):\n",
    "        '''\n",
    "            Removes any web link that follows a particular default expression,\n",
    "            present in the text.\n",
    "        '''\n",
    "\n",
    "        web_links_regex = re.compile(TextCleaningUtils.cleaning_regex_map['web_links'])\n",
    "        text = TextCleaningUtils.clean_text_from_regex(text, web_links_regex)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_twitter_handles(text):\n",
    "        '''\n",
    "            Removes any twitter handle present in the text.\n",
    "        '''\n",
    "\n",
    "        twitter_handles_regex = re.compile(TextCleaningUtils.cleaning_regex_map['twitter_handles'])\n",
    "        text = TextCleaningUtils.clean_text_from_regex(text, twitter_handles_regex)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_redundant_newlines(text):\n",
    "        '''\n",
    "            Removes any redundant new line present in the text.\n",
    "        '''\n",
    "\n",
    "        redundant_newlines_regex = re.compile(\n",
    "            TextCleaningUtils.cleaning_regex_map['redundant_newlines'])\n",
    "        text = TextCleaningUtils.clean_text_from_regex(text, redundant_newlines_regex)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuations(text):\n",
    "        '''\n",
    "            Removes any punctuation that follows the default expression, in the text.\n",
    "        '''\n",
    "\n",
    "        remove_punctuations_regex = re.compile(TextCleaningUtils.cleaning_regex_map['punctuations'])\n",
    "        text = TextCleaningUtils.clean_text_from_regex(text, remove_punctuations_regex)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_exaggerated_words(text):\n",
    "        '''\n",
    "            Removes any exaggerated word present in the text.\n",
    "        '''\n",
    "\n",
    "        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_multiple_chars(text):\n",
    "        '''\n",
    "            Replaces multiple characters present in the text.\n",
    "        '''\n",
    "\n",
    "        char_list = ['.', '?', '!', '#', '$', '/', '@', '*', '(', ')', '+']\n",
    "        final_text = ''\n",
    "        for i in char_list:\n",
    "            if i in text:\n",
    "                pattern = \"\\\\\" + i + '{2,}'\n",
    "                repl_str = i.replace(\"\\\\\", \"\")\n",
    "                text = re.sub(pattern, repl_str, text)\n",
    "                final_text = ' '.join(text.split())\n",
    "        return final_text\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_sign(text):\n",
    "        '''\n",
    "            Replaces any sign with words like & with 'and', in the text.\n",
    "        '''\n",
    "        sign_list = {'&': ' and ', '/': ' or ', '\\xa0': ' '}\n",
    "        final_text = ''\n",
    "        for i in sign_list:\n",
    "            if i in text:\n",
    "                text = re.sub(i, sign_list[i], text)\n",
    "                final_text = ' '.join(text.split())\n",
    "        return final_text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_accented_char(text):\n",
    "        text = unicodedata.normalize('NFD', text) \\\n",
    "            .encode('ascii', 'ignore') \\\n",
    "            .decode(\"utf-8\")\n",
    "        return str(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_characters(text, replace_map):\n",
    "        '''\n",
    "            Replaces any character custom provided by an user.\n",
    "        '''\n",
    "\n",
    "        for char, replace_val in replace_map.items():\n",
    "            text = text.replace(char, replace_val)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df,col_to_clean):\n",
    "  df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_special_chars)\n",
    "  df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_spaces)\n",
    "  df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_punctuations)\n",
    "  df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_exaggerated_words)\n",
    "  df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_newlines)\n",
    "  df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_twitter_handles)\n",
    "  df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_web_links)\n",
    "  df[col_to_clean] = df[col_to_clean].astype(str)\n",
    "  df[col_to_clean] = df[col_to_clean].str.lower()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_data(train,'text')\n",
    "test = clean_data(test,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train[\"text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"insult\"]].values\n",
    "X_test = test[\"text\"].fillna(\"fillna\").values\n",
    "y_test = test[[\"insult\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "embeddings_index = load_facebook_model(EMBEDDING_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    #embedding_vector = embeddings_index.get(word)\n",
    "    embedding_vector = None\n",
    "    if word in embeddings_index.wv:\n",
    "        embedding_vector = embeddings_index.wv[word]\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = Precision()\n",
    "    p.update_state(y_true, y_pred)\n",
    "    precision = p.result().numpy()\n",
    "    r = Recall()\n",
    "    r.update_state(y_true, y_pred)\n",
    "    recall = r.result().numpy()\n",
    "    return precision, recall, 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "\n",
    "class F1Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = f1(self.y_val, y_pred)\n",
    "            print(score)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[Precision(), Recall()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 6\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8)\n",
    "F1_e = F1Evaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[F1_e], verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "def cross_val(X, Y):\n",
    "    for i in range(5):\n",
    "\n",
    "        print(i)\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.8)\n",
    "        F1_e = F1Evaluation(validation_data=(X_val, y_val), interval=1)\n",
    "        model = get_model()\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[F1_e], verbose=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "def cross_val(X, Y):\n",
    "    for i in range(5):\n",
    "\n",
    "        print(i)\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_test, y_test, train_size=0.8)\n",
    "        F1_e = F1Evaluation(validation_data=(X_val, y_val), interval=1)\n",
    "        model = get_model()\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[F1_e], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cross_val(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "print(f1(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}